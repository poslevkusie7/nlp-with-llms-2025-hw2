{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c468709d",
   "metadata": {},
   "source": [
    "# ANLI Baseline with LLM\n",
    "\n",
    "You have to implement in this notebook a baseline for ANLI classification using an LLM.\n",
    "This baseline must be implemented using DSPy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2cec0d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the DSPy environment with the language model - for grok the parameters must be:\n",
    "# env variable should be in os.environ['XAI_API_KEY']\n",
    "# \"xai/grok-3-mini\"\n",
    "import os\n",
    "import dspy\n",
    "\n",
    "lm = dspy.LM('xai/grok-3-mini', api_key=os.environ['XAI_API_KEY'])\n",
    "# for ollama \n",
    "# lm = dspy.LM('ollama_chat/devstral', api_base='http://localhost:11434', api_key='')\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b60da44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "## Implement the DSPy classifier program.\n",
    "\n",
    "class ANLIClassifier(dspy.Signature):\n",
    "    \"\"\"Natural Language Inference task: Given a premise and hypothesis, determine the relationship.\"\"\"\n",
    "    \n",
    "    premise = dspy.InputField(desc=\"The premise statement\")\n",
    "    hypothesis = dspy.InputField(desc=\"The hypothesis statement to check against the premise\")\n",
    "    label = dspy.OutputField(desc=\"The relationship: 'entailment', 'neutral', or 'contradiction'\")\n",
    "\n",
    "class ANLIPredictor(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.classify = dspy.ChainOfThought(ANLIClassifier)\n",
    "    \n",
    "    def forward(self, premise, hypothesis):\n",
    "        result = self.classify(premise=premise, hypothesis=hypothesis)\n",
    "        # Ensure the label is one of the valid options\n",
    "        label = result.label.lower().strip()\n",
    "        if label not in ['entailment', 'neutral', 'contradiction']:\n",
    "            # Default to neutral if unclear\n",
    "            label = 'neutral'\n",
    "        return dspy.Prediction(label=label, reasoning=result.rationale if hasattr(result, 'rationale') else \"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ab422d",
   "metadata": {},
   "source": [
    "## Load ANLI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0438789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"facebook/anli\")\n",
    "dataset = dataset.filter(lambda x: x['reason'] != None and x['reason'] != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e59927ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 2923\n",
       "    })\n",
       "    dev_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    train_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 4861\n",
       "    })\n",
       "    dev_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    train_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 13375\n",
       "    })\n",
       "    dev_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "    test_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8e1258",
   "metadata": {},
   "source": [
    "## Evaluate Metrics\n",
    "\n",
    "Let's use the huggingface `evaluate` package to compute the performance of the baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e2e9027",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "accuracy = load(\"accuracy\")\n",
    "precision = load(\"precision\")\n",
    "recall = load(\"recall\")\n",
    "f1 = load(\"f1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ab24e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d04f0c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6666666666666666,\n",
       " 'f1': 0.6666666666666666,\n",
       " 'precision': 1.0,\n",
       " 'recall': 0.5}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_metrics.compute(predictions=[0, 1, 0], references=[0, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0909d58b",
   "metadata": {},
   "source": [
    "## Your Turn\n",
    "\n",
    "Compute the classification metrics on the baseline LLM model on each test section of the ANLI dataset for samples that have a non-empty 'reason' field.\n",
    "\n",
    "You also must show a comparison between the DeBERTa baseline model and this LLM baseline model. The comparison metric should compute the agreement between the two models:\n",
    "* On how many samples they are both correct [Correct]\n",
    "* On how many samples Model1 is correct and Model2 is incorrect [Correct1]\n",
    "* On how many samples Model1 is incorrect and Model2 is correct [Correct2]\n",
    "* On how many samples both are incorrect [Incorrect]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0d735fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing datasets...\n",
      "Dev R3 examples for optimization: 50\n",
      "Test R3 examples for evaluation: 1200\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load and filter dataset\n",
    "dataset = load_dataset(\"facebook/anli\")\n",
    "dataset = dataset.filter(lambda x: x['reason'] != None and x['reason'] != \"\")\n",
    "\n",
    "# Convert to DSPy format\n",
    "def convert_to_dspy_examples(data_split, sample_size=None):\n",
    "    examples = []\n",
    "    label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "    \n",
    "    data = list(data_split)\n",
    "    if sample_size:\n",
    "        data = random.sample(data, min(sample_size, len(data)))\n",
    "    \n",
    "    for example in data:\n",
    "        examples.append(dspy.Example(\n",
    "            premise=example['premise'],\n",
    "            hypothesis=example['hypothesis'],\n",
    "            label=label_names[example['label']]\n",
    "        ).with_inputs('premise', 'hypothesis'))\n",
    "    \n",
    "    return examples\n",
    "\n",
    "# Prepare training and test data\n",
    "print(\"Preparing datasets...\")\n",
    "dev_r3_examples = convert_to_dspy_examples(dataset['dev_r3'], sample_size=50)  # Optimize on 50 examples\n",
    "test_r3_examples = convert_to_dspy_examples(dataset['test_r3'])\n",
    "\n",
    "print(f\"Dev R3 examples for optimization: {len(dev_r3_examples)}\")\n",
    "print(f\"Test R3 examples for evaluation: {len(test_r3_examples)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "71318290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing predictor before optimization...\n",
      "Example: Healthier Life<br>Maddie wanted to lead a healthie... | Maddie liked the idea of being healthy...\n",
      "Predicted: entailment, Actual: entailment\n",
      "\n",
      "Optimizing predictor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 10/50 [01:09<04:37,  6.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 8 full traces after 10 examples for up to 1 rounds, amounting to 10 attempts.\n",
      "Optimization complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from dspy.teleprompt import BootstrapFewShot\n",
    "\n",
    "# Initialize predictor\n",
    "predictor = ANLIPredictor()\n",
    "\n",
    "# Test before optimization\n",
    "print(\"Testing predictor before optimization...\")\n",
    "test_example = dev_r3_examples[0]\n",
    "result = predictor(premise=test_example.premise, hypothesis=test_example.hypothesis)\n",
    "print(f\"Example: {test_example.premise[:50]}... | {test_example.hypothesis[:50]}...\")\n",
    "print(f\"Predicted: {result.label}, Actual: {test_example.label}\")\n",
    "\n",
    "# Set up optimizer\n",
    "def validate_prediction(example, pred, trace=None):\n",
    "    return pred.label == example.label\n",
    "\n",
    "# Optimize the model\n",
    "print(\"\\nOptimizing predictor...\")\n",
    "teleprompter = BootstrapFewShot(metric=validate_prediction, max_bootstrapped_demos=8, max_labeled_demos=8)\n",
    "optimized_predictor = teleprompter.compile(predictor, trainset=dev_r3_examples)\n",
    "\n",
    "print(\"Optimization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d66f265",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_llm_on_dataset(predictor, examples):\n",
    "    results = []\n",
    "    \n",
    "    for example in tqdm(examples, desc=\"Evaluating\"):\n",
    "        try:\n",
    "            prediction = predictor(premise=example.premise, hypothesis=example.hypothesis)\n",
    "            results.append({\n",
    "                'premise': example.premise,\n",
    "                'hypothesis': example.hypothesis,\n",
    "                'pred_label': prediction.label,\n",
    "                'gold_label': example.label,\n",
    "                'correct': prediction.label == example.label\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing example: {e}\")\n",
    "            # Add a default neutral prediction for failed cases\n",
    "            results.append({\n",
    "                'premise': example.premise,\n",
    "                'hypothesis': example.hypothesis,\n",
    "                'pred_label': 'neutral',\n",
    "                'gold_label': example.label,\n",
    "                'correct': 'neutral' == example.label\n",
    "            })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e9daa8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating optimized LLM model on test_r3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1200/1200 [1:56:19<00:00,  5.82s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LLM Model Metrics on test_r3 (1200 examples) ===\n",
      "Accuracy : 0.7342\n",
      "Precision: 0.7540\n",
      "Recall   : 0.7340\n",
      "F1       : 0.7382\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating optimized LLM model on test_r3...\")\n",
    "llm_results = evaluate_llm_on_dataset(optimized_predictor, test_r3_examples)\n",
    "\n",
    "# Compute metrics\n",
    "from evaluate import load\n",
    "accuracy = load(\"accuracy\")\n",
    "precision = load(\"precision\")\n",
    "recall = load(\"recall\")\n",
    "f1 = load(\"f1\")\n",
    "\n",
    "label2id = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n",
    "\n",
    "def compute_metrics(results):\n",
    "    preds = [label2id[r['pred_label']] for r in results]\n",
    "    refs = [label2id[r['gold_label']] for r in results]\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy.compute(predictions=preds, references=refs)[\"accuracy\"],\n",
    "        \"precision\": precision.compute(predictions=preds, references=refs, average=\"macro\")[\"precision\"],\n",
    "        \"recall\": recall.compute(predictions=preds, references=refs, average=\"macro\")[\"recall\"],\n",
    "        \"f1\": f1.compute(predictions=preds, references=refs, average=\"macro\")[\"f1\"],\n",
    "    }\n",
    "\n",
    "llm_metrics = compute_metrics(llm_results)\n",
    "print(f\"\\n=== LLM Model Metrics on test_r3 ({len(llm_results)} examples) ===\")\n",
    "print(f\"Accuracy : {llm_metrics['accuracy']:.4f}\")\n",
    "print(f\"Precision: {llm_metrics['precision']:.4f}\")\n",
    "print(f\"Recall   : {llm_metrics['recall']:.4f}\")\n",
    "print(f\"F1       : {llm_metrics['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ed889d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_deberta_results():\n",
    "    import json\n",
    "    import os\n",
    "    \n",
    "    # Try to load the saved results\n",
    "    json_file = 'deberta_test_r3_results.json'\n",
    "    csv_file = 'deberta_test_r3_results.csv'\n",
    "    \n",
    "    if os.path.exists(json_file):\n",
    "        print(f\"Loading DeBERTa results from {json_file}\")\n",
    "        with open(json_file, 'r') as f:\n",
    "            deberta_results = json.load(f)\n",
    "        print(f\"Loaded {len(deberta_results)} DeBERTa results\")\n",
    "        return deberta_results\n",
    "    \n",
    "    elif os.path.exists(csv_file):\n",
    "        print(f\"Loading DeBERTa results from {csv_file}\")\n",
    "        import pandas as pd\n",
    "        df = pd.read_csv(csv_file)\n",
    "        deberta_results = df.to_dict('records')\n",
    "        print(f\"Loaded {len(deberta_results)} DeBERTa results\")\n",
    "        return deberta_results\n",
    "    \n",
    "    else:\n",
    "        print(\"ERROR: No DeBERTa results file found!\")\n",
    "        print(\"Please run the DeBERTa code first and save results using:\")\n",
    "        print(\"- deberta_test_r3_results.json\")\n",
    "        print(\"- deberta_test_r3_results.csv\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398b993b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model Comparison ===\n",
      "To complete the comparison, you need to:\n",
      "1. Run your DeBERTa baseline on the same test_r3 examples\n",
      "2. Save those results and load them here\n",
      "3. Then run the compare_models function\n",
      "Loading DeBERTa results from deberta_test_r3_results.json\n",
      "Loaded 1200 DeBERTa results\n",
      "Both Correct: 479 (0.399)\n",
      "LLM Correct, DeBERTa Wrong: 402 (0.335)\n",
      "DeBERTa Correct, LLM Wrong: 115 (0.096)\n",
      "Both Incorrect: 204 (0.170)\n"
     ]
    }
   ],
   "source": [
    "def compare_models(llm_results, deberta_results):\n",
    "    \n",
    "    if len(deberta_results) == 0:\n",
    "        print(\"No DeBERTa results provided. Please run DeBERTa baseline first.\")\n",
    "        return None\n",
    "    \n",
    "    assert len(llm_results) == len(deberta_results), \"Results must have same length\"\n",
    "    \n",
    "    comparison = {\n",
    "        'both_correct': 0,      # Both models correct\n",
    "        'llm_correct_deberta_wrong': 0,  # LLM correct, DeBERTa wrong\n",
    "        'deberta_correct_llm_wrong': 0,  # DeBERTa correct, LLM wrong\n",
    "        'both_incorrect': 0     # Both models incorrect\n",
    "    }\n",
    "    \n",
    "    for llm_pred, deberta_pred in zip(llm_results, deberta_results):\n",
    "        llm_correct = llm_pred['correct']\n",
    "        deberta_correct = deberta_pred['correct']\n",
    "        \n",
    "        if llm_correct and deberta_correct:\n",
    "            comparison['both_correct'] += 1\n",
    "        elif llm_correct and not deberta_correct:\n",
    "            comparison['llm_correct_deberta_wrong'] += 1\n",
    "        elif not llm_correct and deberta_correct:\n",
    "            comparison['deberta_correct_llm_wrong'] += 1\n",
    "        else:\n",
    "            comparison['both_incorrect'] += 1\n",
    "    \n",
    "    return comparison\n",
    "\n",
    "print(\"\\n=== Model Comparison ===\")\n",
    "\n",
    "deberta_results = load_deberta_results()  \n",
    "comparison = compare_models(llm_results, deberta_results)\n",
    "if comparison:\n",
    "    total = len(llm_results)\n",
    "    print(f\"Both Correct: {comparison['both_correct']} ({comparison['both_correct']/total:.3f})\")\n",
    "    print(f\"LLM Correct, DeBERTa Wrong: {comparison['llm_correct_deberta_wrong']} ({comparison['llm_correct_deberta_wrong']/total:.3f})\")\n",
    "    print(f\"DeBERTa Correct, LLM Wrong: {comparison['deberta_correct_llm_wrong']} ({comparison['deberta_correct_llm_wrong']/total:.3f})\")\n",
    "    print(f\"Both Incorrect: {comparison['both_incorrect']} ({comparison['both_incorrect']/total:.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e801813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample LLM Predictions ===\n",
      "\n",
      "Example 1:\n",
      "Premise: It is Sunday today, let's take a look at the most popular posts of the last couple of days. Most of ...\n",
      "Hypothesis: The day of the passage is usually when Christians praise the lord together...\n",
      "Predicted: neutral\n",
      "Gold: entailment\n",
      "Correct: False\n",
      "\n",
      "Example 2:\n",
      "Premise: By The Associated Press WELLINGTON, New Zealand (AP) — All passengers and crew have survived a crash...\n",
      "Hypothesis: No children were killed in the accident....\n",
      "Predicted: entailment\n",
      "Gold: entailment\n",
      "Correct: True\n",
      "\n",
      "Example 3:\n",
      "Premise: Tokyo - Food group Nestle is seeking to lure Japanese holiday shoppers with a taste for fine snackin...\n",
      "Hypothesis: Japanese like kit kat. ...\n",
      "Predicted: entailment\n",
      "Gold: entailment\n",
      "Correct: True\n",
      "\n",
      "Example 4:\n",
      "Premise: Governor Greg Abbott has called for a statewide show of support for law enforcement Friday, July 7. ...\n",
      "Hypothesis: Law enforcement officers and the people at the Travis St. memorial do not show their support at the ...\n",
      "Predicted: entailment\n",
      "Gold: entailment\n",
      "Correct: True\n",
      "\n",
      "Example 5:\n",
      "Premise: Sept 4 (Reuters) - J.P. Morgan Asset Management, a unit of JPMorgan Chase & Co, said it appointed Pi...\n",
      "Hypothesis: Pietro Grassano was once the country head for France....\n",
      "Predicted: contradiction\n",
      "Gold: entailment\n",
      "Correct: False\n",
      "\n",
      "Implementation complete! Remember to:\n",
      "1. Run your DeBERTa model on the same test_r3 examples\n",
      "2. Implement the load_deberta_results() function\n",
      "3. Run the model comparison\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n=== Sample LLM Predictions ===\")\n",
    "for i in range(min(5, len(llm_results))):\n",
    "    result = llm_results[i]\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Premise: {result['premise'][:100]}...\")\n",
    "    print(f\"Hypothesis: {result['hypothesis'][:100]}...\")\n",
    "    print(f\"Predicted: {result['pred_label']}\")\n",
    "    print(f\"Gold: {result['gold_label']}\")\n",
    "    print(f\"Correct: {result['correct']}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
