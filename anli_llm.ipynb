{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c468709d",
   "metadata": {},
   "source": [
    "# ANLI with LLM\n",
    "\n",
    "You have to implement in this notebook a better ANLI classifier using an LLM.\n",
    "This classifier must be implemented using DSPy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cec0d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the DSPy environment with the language model - for grok the parameters must be:\n",
    "# env variable should be in os.environ['XAI_API_KEY']\n",
    "# \"xai/grok-3-mini\"\n",
    "import os\n",
    "import dspy\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "lm = dspy.LM('xai/grok-3-mini', api_key=os.environ['XAI_API_KEY'])\n",
    "# for ollama \n",
    "# lm = dspy.LM('ollama_chat/devstral', api_base='http://localhost:11434', api_key='')\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec0d1f85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3f9db475bcd49f79f9dbf42f83411be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc49226763a2487b8cbec2ed0cd42e23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0bec46f8650415d970e5ab3c456160a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "200f6857cdc34d2595ab6a003ca71a57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0292de1eeef4928b3e5bf079f660568",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8efc1538b3c48a997f74000e40ec634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e65a8c975724ebfaa304f433cd349b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b6ee293656c450aac05ad45b5f8ad7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47d9d668b83f4ae6b1916e6dda24e856",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acddb9fc6a724286aa254c83b7e1edf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0b4007ebfc3479b950cff2e68f5898b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded sentence transformer model for similarity computation\n"
     ]
    }
   ],
   "source": [
    "# Load the sentence transformer model for similarity computation - I dont know if this is needed\n",
    "similarity_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "print(\"Loaded sentence transformer model for similarity computation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b60da44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "## Implement the DSPy classifier program.\n",
    "class ANLIJointSignature(dspy.Signature):\n",
    "    \"\"\"Natural Language Inference with Chain-of-Thought explanation. \n",
    "    Given premise and hypothesis, provide both explanation and classification.\"\"\"\n",
    "    \n",
    "    premise = dspy.InputField(desc=\"The premise statement\")\n",
    "    hypothesis = dspy.InputField(desc=\"The hypothesis statement to check against premise\")\n",
    "    explanation = dspy.OutputField(desc=\"Step-by-step explanation of the relationship between premise and hypothesis\")\n",
    "    label = dspy.OutputField(desc=\"Classification: 'entailment', 'neutral', or 'contradiction'\")\n",
    "\n",
    "# Pipeline approach: First explanation, then label\n",
    "class ANLIExplanationSignature(dspy.Signature):\n",
    "    \"\"\"Generate explanation for the relationship between premise and hypothesis.\"\"\"\n",
    "    \n",
    "    premise = dspy.InputField(desc=\"The premise statement\")\n",
    "    hypothesis = dspy.InputField(desc=\"The hypothesis statement\")\n",
    "    explanation = dspy.OutputField(desc=\"Detailed explanation of how the hypothesis relates to the premise\")\n",
    "\n",
    "class ANLILabelFromExplanationSignature(dspy.Signature):\n",
    "    \"\"\"Given premise, hypothesis and explanation, determine the NLI label.\"\"\"\n",
    "    \n",
    "    premise = dspy.InputField(desc=\"The premise statement\")\n",
    "    hypothesis = dspy.InputField(desc=\"The hypothesis statement\")\n",
    "    explanation = dspy.InputField(desc=\"Explanation of their relationship\")\n",
    "    label = dspy.OutputField(desc=\"Classification: 'entailment', 'neutral', or 'contradiction'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ab422d",
   "metadata": {},
   "source": [
    "## Load ANLI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0438789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"facebook/anli\")\n",
    "dataset = dataset.filter(lambda x: x['reason'] != None and x['reason'] != \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8e1258",
   "metadata": {},
   "source": [
    "## Evaluate Metrics\n",
    "\n",
    "Let's use the huggingface `evaluate` package to compute the performance of the baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e2e9027",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "accuracy = load(\"accuracy\")\n",
    "precision = load(\"precision\")\n",
    "recall = load(\"recall\")\n",
    "f1 = load(\"f1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ab24e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0909d58b",
   "metadata": {},
   "source": [
    "## Your Turn\n",
    "\n",
    "Compute the classification metrics on the baseline LLM model on each test section of the ANLI dataset for samples that have a non-empty 'reason' field.\n",
    "\n",
    "You also must show a comparison between the DeBERTa baseline model and this LLM baseline model. The comparison metric should compute the agreement between the two models:\n",
    "* On how many samples they are both correct [Correct]\n",
    "* On how many samples Model1 is correct and Model2 is incorrect [Correct1]\n",
    "* On how many samples Model1 is incorrect and Model2 is correct [Correct2]\n",
    "* On how many samples both are incorrect [Incorrect]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d85dadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity(text1, text2):\n",
    "    \"\"\"Compute cosine similarity between two texts using sentence transformers\"\"\"\n",
    "    embeddings = similarity_model.encode([text1, text2])\n",
    "    similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
    "    return similarity\n",
    "\n",
    "def compute_explanation_relevance(premise, hypothesis, explanation, human_explanation=None):\n",
    "    \"\"\"\n",
    "    Compute relevance scores for explanation:\n",
    "    1. Similarity to (premise, hypothesis) concatenated\n",
    "    2. Similarity to human explanation (if available)\n",
    "    \"\"\"\n",
    "    premise_hypothesis = f\"{premise} {hypothesis}\"\n",
    "    \n",
    "    # Similarity between explanation and (premise, hypothesis)\n",
    "    relevance_score = compute_similarity(explanation, premise_hypothesis)\n",
    "    \n",
    "    # Similarity to human explanation if available\n",
    "    human_similarity = None\n",
    "    if human_explanation:\n",
    "        human_similarity = compute_similarity(explanation, human_explanation)\n",
    "    \n",
    "    # Baseline: similarity between (premise, hypothesis) and human explanation\n",
    "    baseline_similarity = None\n",
    "    if human_explanation:\n",
    "        baseline_similarity = compute_similarity(premise_hypothesis, human_explanation)\n",
    "    \n",
    "    return {\n",
    "        'relevance_to_input': relevance_score,\n",
    "        'similarity_to_human': human_similarity,\n",
    "        'baseline_human_similarity': baseline_similarity\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52768f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANLIJointPredictor(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.predictor = dspy.ChainOfThought(ANLIJointSignature)\n",
    "    \n",
    "    def forward(self, premise, hypothesis, human_explanation=None):\n",
    "        result = self.predictor(premise=premise, hypothesis=hypothesis)\n",
    "        \n",
    "        # Clean up the label\n",
    "        label = result.label.lower().strip()\n",
    "        if label not in ['entailment', 'neutral', 'contradiction']:\n",
    "            label = 'neutral'\n",
    "        \n",
    "        # Compute relevance scores\n",
    "        relevance_scores = compute_explanation_relevance(\n",
    "            premise, hypothesis, result.explanation, human_explanation\n",
    "        )\n",
    "        \n",
    "        return dspy.Prediction(\n",
    "            label=label,\n",
    "            explanation=result.explanation,\n",
    "            relevance_scores=relevance_scores\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2d3206d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANLIPipelinePredictor(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.explanation_generator = dspy.ChainOfThought(ANLIExplanationSignature)\n",
    "        self.label_classifier = dspy.ChainOfThought(ANLILabelFromExplanationSignature)\n",
    "    \n",
    "    def forward(self, premise, hypothesis, human_explanation=None):\n",
    "        # Step 1: Generate explanation\n",
    "        explanation_result = self.explanation_generator(premise=premise, hypothesis=hypothesis)\n",
    "        \n",
    "        # Step 2: Generate label from explanation\n",
    "        label_result = self.label_classifier(\n",
    "            premise=premise, \n",
    "            hypothesis=hypothesis, \n",
    "            explanation=explanation_result.explanation\n",
    "        )\n",
    "        \n",
    "        # Clean up the label\n",
    "        label = label_result.label.lower().strip()\n",
    "        if label not in ['entailment', 'neutral', 'contradiction']:\n",
    "            label = 'neutral'\n",
    "        \n",
    "        # Compute relevance scores\n",
    "        relevance_scores = compute_explanation_relevance(\n",
    "            premise, hypothesis, explanation_result.explanation, human_explanation\n",
    "        )\n",
    "        \n",
    "        return dspy.Prediction(\n",
    "            label=label,\n",
    "            explanation=explanation_result.explanation,\n",
    "            relevance_scores=relevance_scores\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f71d1eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExplanationRefiner(dspy.Module):\n",
    "    def __init__(self, threshold=0.5):\n",
    "        super().__init__()\n",
    "        self.refiner = dspy.Refine()\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def forward(self, predictor, premise, hypothesis, human_explanation=None):\n",
    "        \"\"\"Refine prediction if explanation relevance is below threshold\"\"\"\n",
    "        initial_pred = predictor(premise=premise, hypothesis=hypothesis, human_explanation=human_explanation)\n",
    "        \n",
    "        # Check if refinement is needed based on relevance score\n",
    "        relevance = initial_pred.relevance_scores['relevance_to_input']\n",
    "        \n",
    "        if relevance < self.threshold:\n",
    "            # Attempt to refine\n",
    "            refined_pred = self.refiner(\n",
    "                predictor=predictor,\n",
    "                premise=premise,\n",
    "                hypothesis=hypothesis,\n",
    "                feedback=f\"The explanation should be more relevant to the premise and hypothesis. Current relevance: {relevance:.3f}\"\n",
    "            )\n",
    "            return refined_pred\n",
    "        \n",
    "        return initial_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "063fd81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dev_r3 data for comparison...\n",
      "Prepared 50 examples for evaluation\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"facebook/anli\")\n",
    "dataset = dataset.filter(lambda x: x['reason'] != None and x['reason'] != \"\")\n",
    "\n",
    "# Prepare dev_r3 data\n",
    "def prepare_evaluation_data(split_name, sample_size=100):\n",
    "    \"\"\"Prepare data for evaluation\"\"\"\n",
    "    examples = []\n",
    "    label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "    \n",
    "    data = list(dataset[split_name])\n",
    "    if sample_size and len(data) > sample_size:\n",
    "        data = random.sample(data, sample_size)\n",
    "    \n",
    "    for example in data:\n",
    "        examples.append({\n",
    "            'premise': example['premise'],\n",
    "            'hypothesis': example['hypothesis'],\n",
    "            'gold_label': label_names[example['label']],\n",
    "            'human_explanation': example['reason']\n",
    "        })\n",
    "    \n",
    "    return examples\n",
    "\n",
    "# Prepare evaluation data\n",
    "print(\"Preparing dev_r3 data for comparison...\")\n",
    "eval_data = prepare_evaluation_data('dev_r3', sample_size=50)  # Adjust sample size as needed\n",
    "print(f\"Prepared {len(eval_data)} examples for evaluation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "594ab0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_approach(predictor, data, approach_name):\n",
    "    \"\"\"Evaluate a single approach (joint or pipeline)\"\"\"\n",
    "    results = []\n",
    "    relevance_scores = []\n",
    "    human_similarities = []\n",
    "    baseline_similarities = []\n",
    "    \n",
    "    print(f\"Evaluating {approach_name} approach...\")\n",
    "    \n",
    "    for example in tqdm(data, desc=f\"{approach_name} evaluation\"):\n",
    "        try:\n",
    "            prediction = predictor(\n",
    "                premise=example['premise'],\n",
    "                hypothesis=example['hypothesis'],\n",
    "                human_explanation=example['human_explanation']\n",
    "            )\n",
    "            \n",
    "            result = {\n",
    "                'premise': example['premise'],\n",
    "                'hypothesis': example['hypothesis'],\n",
    "                'pred_label': prediction.label,\n",
    "                'gold_label': example['gold_label'],\n",
    "                'pred_explanation': prediction.explanation,\n",
    "                'human_explanation': example['human_explanation'],\n",
    "                'correct': prediction.label == example['gold_label'],\n",
    "                'relevance_scores': prediction.relevance_scores\n",
    "            }\n",
    "            \n",
    "            results.append(result)\n",
    "            \n",
    "            # Collect scores for analysis\n",
    "            relevance_scores.append(prediction.relevance_scores['relevance_to_input'])\n",
    "            if prediction.relevance_scores['similarity_to_human']:\n",
    "                human_similarities.append(prediction.relevance_scores['similarity_to_human'])\n",
    "            if prediction.relevance_scores['baseline_human_similarity']:\n",
    "                baseline_similarities.append(prediction.relevance_scores['baseline_human_similarity'])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing example: {e}\")\n",
    "            # Add default result for failed cases\n",
    "            results.append({\n",
    "                'premise': example['premise'],\n",
    "                'hypothesis': example['hypothesis'],\n",
    "                'pred_label': 'neutral',\n",
    "                'gold_label': example['gold_label'],\n",
    "                'pred_explanation': 'Error in processing',\n",
    "                'human_explanation': example['human_explanation'],\n",
    "                'correct': 'neutral' == example['gold_label'],\n",
    "                'relevance_scores': {'relevance_to_input': 0.0, 'similarity_to_human': 0.0, 'baseline_human_similarity': 0.0}\n",
    "            })\n",
    "    \n",
    "    return results, relevance_scores, human_similarities, baseline_similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8348e8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Joint approach...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Joint evaluation: 100%|██████████| 50/50 [07:25<00:00,  8.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Pipeline approach...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pipeline evaluation: 100%|██████████| 50/50 [10:25<00:00, 12.50s/it]\n"
     ]
    }
   ],
   "source": [
    "# Initialize predictors\n",
    "joint_predictor = ANLIJointPredictor()\n",
    "pipeline_predictor = ANLIPipelinePredictor()\n",
    "\n",
    "# Evaluate both approaches\n",
    "joint_results, joint_relevance, joint_human_sim, joint_baseline = evaluate_approach(\n",
    "    joint_predictor, eval_data, \"Joint\"\n",
    ")\n",
    "\n",
    "pipeline_results, pipeline_relevance, pipeline_human_sim, pipeline_baseline = evaluate_approach(\n",
    "    pipeline_predictor, eval_data, \"Pipeline\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "638e8f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "accuracy = load(\"accuracy\")\n",
    "precision = load(\"precision\")\n",
    "recall = load(\"recall\")\n",
    "f1 = load(\"f1\")\n",
    "\n",
    "label2id = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n",
    "\n",
    "def compute_approach_metrics(results):\n",
    "    \"\"\"Compute classification metrics for an approach\"\"\"\n",
    "    preds = [label2id[r['pred_label']] for r in results]\n",
    "    refs = [label2id[r['gold_label']] for r in results]\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy.compute(predictions=preds, references=refs)[\"accuracy\"],\n",
    "        \"precision\": precision.compute(predictions=preds, references=refs, average=\"macro\")[\"precision\"],\n",
    "        \"recall\": recall.compute(predictions=preds, references=refs, average=\"macro\")[\"recall\"],\n",
    "        \"f1\": f1.compute(predictions=preds, references=refs, average=\"macro\")[\"f1\"],\n",
    "    }\n",
    "\n",
    "def compute_explanation_metrics(relevance_scores, human_similarities, baseline_similarities):\n",
    "    \"\"\"Compute explanation quality metrics\"\"\"\n",
    "    return {\n",
    "        \"avg_relevance_to_input\": np.mean(relevance_scores),\n",
    "        \"std_relevance_to_input\": np.std(relevance_scores),\n",
    "        \"avg_similarity_to_human\": np.mean(human_similarities) if human_similarities else 0.0,\n",
    "        \"std_similarity_to_human\": np.std(human_similarities) if human_similarities else 0.0,\n",
    "        \"avg_baseline_similarity\": np.mean(baseline_similarities) if baseline_similarities else 0.0\n",
    "    }\n",
    "\n",
    "# Compute metrics for both approaches\n",
    "joint_class_metrics = compute_approach_metrics(joint_results)\n",
    "pipeline_class_metrics = compute_approach_metrics(pipeline_results)\n",
    "\n",
    "joint_exp_metrics = compute_explanation_metrics(joint_relevance, joint_human_sim, joint_baseline)\n",
    "pipeline_exp_metrics = compute_explanation_metrics(pipeline_relevance, pipeline_human_sim, pipeline_baseline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63a6883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RESULTS COMPARISON: Joint vs Pipeline Approaches\n",
      "============================================================\n",
      "\n",
      "CLASSIFICATION METRICS    Joint      Pipeline   Difference  \n",
      "------------------------------------------------------------\n",
      "Accuracy                  0.6200     0.6000     +0.0200\n",
      "Precision                 0.6343     0.6093     +0.0251\n",
      "Recall                    0.6263     0.6181     +0.0082\n",
      "F1                        0.6235     0.6093     +0.0142\n",
      "\n",
      "EXPLANATION METRICS       Joint      Pipeline   Difference  \n",
      "------------------------------------------------------------\n",
      "Relevance to Input        0.6467     0.6586     -0.0119\n",
      "Similarity to Human       0.5279     0.5204     +0.0075\n",
      "Baseline Human Sim        0.4087     0.4087     N/A         \n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"RESULTS COMPARISON: Joint vs Pipeline Approaches\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n{'CLASSIFICATION METRICS':<25} {'Joint':<10} {'Pipeline':<10} {'Difference':<12}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Accuracy':<25} {joint_class_metrics['accuracy']:<10.4f} {pipeline_class_metrics['accuracy']:<10.4f} {joint_class_metrics['accuracy']-pipeline_class_metrics['accuracy']:+.4f}\")\n",
    "print(f\"{'Precision':<25} {joint_class_metrics['precision']:<10.4f} {pipeline_class_metrics['precision']:<10.4f} {joint_class_metrics['precision']-pipeline_class_metrics['precision']:+.4f}\")\n",
    "print(f\"{'Recall':<25} {joint_class_metrics['recall']:<10.4f} {pipeline_class_metrics['recall']:<10.4f} {joint_class_metrics['recall']-pipeline_class_metrics['recall']:+.4f}\")\n",
    "print(f\"{'F1':<25} {joint_class_metrics['f1']:<10.4f} {pipeline_class_metrics['f1']:<10.4f} {joint_class_metrics['f1']-pipeline_class_metrics['f1']:+.4f}\")\n",
    "\n",
    "print(f\"\\n{'EXPLANATION METRICS':<25} {'Joint':<10} {'Pipeline':<10} {'Difference':<12}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Relevance to Input':<25} {joint_exp_metrics['avg_relevance_to_input']:<10.4f} {pipeline_exp_metrics['avg_relevance_to_input']:<10.4f} {joint_exp_metrics['avg_relevance_to_input']-pipeline_exp_metrics['avg_relevance_to_input']:+.4f}\")\n",
    "print(f\"{'Similarity to Human':<25} {joint_exp_metrics['avg_similarity_to_human']:<10.4f} {pipeline_exp_metrics['avg_similarity_to_human']:<10.4f} {joint_exp_metrics['avg_similarity_to_human']-pipeline_exp_metrics['avg_similarity_to_human']:+.4f}\")\n",
    "print(f\"{'Baseline Human Sim':<25} {joint_exp_metrics['avg_baseline_similarity']:<10.4f} {pipeline_exp_metrics['avg_baseline_similarity']:<10.4f} {'N/A':<12}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f035f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DETAILED ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Found 6 examples where approaches disagree\n",
      "\n",
      "Example of disagreement:\n",
      "Premise: Image copyright Reuters Britain's Mark Cavendish pulled out of the Tour de France after breaking his...\n",
      "Hypothesis: Mark was born in the late eighties ...\n",
      "Gold Label: entailment\n",
      "Joint Prediction: contradiction\n",
      "Pipeline Prediction: entailment\n",
      "Joint Explanation: 1. The premise explicitly states that Mark Cavendish is 32 years old at the time of the events described, which include his crash in the Tour de France and his participation in the 2016 Rio Olympics. ...\n",
      "Pipeline Explanation: The premise describes Mark Cavendish as a 32-year-old cyclist who experienced a crash during the Tour de France, with references to his achievements like winning a silver medal at the 2016 Rio Olympic...\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'DETAILED ANALYSIS'}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Show examples where approaches differ\n",
    "different_predictions = []\n",
    "for joint_res, pipeline_res in zip(joint_results, pipeline_results):\n",
    "    if joint_res['pred_label'] != pipeline_res['pred_label']:\n",
    "        different_predictions.append({\n",
    "            'premise': joint_res['premise'],\n",
    "            'hypothesis': joint_res['hypothesis'],\n",
    "            'gold_label': joint_res['gold_label'],\n",
    "            'joint_pred': joint_res['pred_label'],\n",
    "            'pipeline_pred': pipeline_res['pred_label'],\n",
    "            'joint_explanation': joint_res['pred_explanation'],\n",
    "            'pipeline_explanation': pipeline_res['pred_explanation']\n",
    "        })\n",
    "\n",
    "print(f\"\\nFound {len(different_predictions)} examples where approaches disagree\")\n",
    "\n",
    "if different_predictions:\n",
    "    print(f\"\\nExample of disagreement:\")\n",
    "    example = different_predictions[0]\n",
    "    print(f\"Premise: {example['premise'][:100]}...\")\n",
    "    print(f\"Hypothesis: {example['hypothesis'][:100]}...\")\n",
    "    print(f\"Gold Label: {example['gold_label']}\")\n",
    "    print(f\"Joint Prediction: {example['joint_pred']}\")\n",
    "    print(f\"Pipeline Prediction: {example['pipeline_pred']}\")\n",
    "    print(f\"Joint Explanation: {example['joint_explanation'][:200]}...\")\n",
    "    print(f\"Pipeline Explanation: {example['pipeline_explanation'][:200]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c5d03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "THRESHOLD ANALYSIS\n",
      "========================================\n",
      "Joint Relevance - Mean: 0.6467, Std: 0.1173\n",
      "Pipeline Relevance - Mean: 0.6586, Std: 0.1201\n",
      "Suggested thresholds for refinement:\n",
      "Joint approach: 0.5294\n",
      "Pipeline approach: 0.5385\n",
      "\n",
      "Relevance vs Correctness Analysis:\n",
      "Joint - Correct: 0.6612, Incorrect: 0.6230\n",
      "Pipeline - Correct: 0.6743, Incorrect: 0.6350\n",
      "\n",
      "CONCLUSION\n",
      "========================================\n",
      "Joint approach performs better in classification accuracy\n",
      "Pipeline approach generates more relevant explanations\n",
      "\n",
      "Implementation complete! This reproduces the experiment from Kavumba et al. (EACL 2023)\n",
      "comparing joint vs pipeline approaches for explanation-enhanced NLI classification.\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'THRESHOLD ANALYSIS'}\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Analyze distribution of relevance scores\n",
    "joint_relevance_array = np.array(joint_relevance)\n",
    "pipeline_relevance_array = np.array(pipeline_relevance)\n",
    "\n",
    "print(f\"Joint Relevance - Mean: {np.mean(joint_relevance_array):.4f}, Std: {np.std(joint_relevance_array):.4f}\")\n",
    "print(f\"Pipeline Relevance - Mean: {np.mean(pipeline_relevance_array):.4f}, Std: {np.std(pipeline_relevance_array):.4f}\")\n",
    "\n",
    "# Suggest thresholds\n",
    "joint_threshold = np.mean(joint_relevance_array) - np.std(joint_relevance_array)\n",
    "pipeline_threshold = np.mean(pipeline_relevance_array) - np.std(pipeline_relevance_array)\n",
    "\n",
    "print(f\"Suggested thresholds for refinement:\")\n",
    "print(f\"Joint approach: {joint_threshold:.4f}\")\n",
    "print(f\"Pipeline approach: {pipeline_threshold:.4f}\")\n",
    "\n",
    "# Show correlation between relevance and correctness\n",
    "joint_correct_relevance = [r['relevance_scores']['relevance_to_input'] for r in joint_results if r['correct']]\n",
    "joint_incorrect_relevance = [r['relevance_scores']['relevance_to_input'] for r in joint_results if not r['correct']]\n",
    "\n",
    "pipeline_correct_relevance = [r['relevance_scores']['relevance_to_input'] for r in pipeline_results if r['correct']]\n",
    "pipeline_incorrect_relevance = [r['relevance_scores']['relevance_to_input'] for r in pipeline_results if not r['correct']]\n",
    "\n",
    "print(f\"\\nRelevance vs Correctness Analysis:\")\n",
    "print(f\"Joint - Correct: {np.mean(joint_correct_relevance):.4f}, Incorrect: {np.mean(joint_incorrect_relevance):.4f}\")\n",
    "print(f\"Pipeline - Correct: {np.mean(pipeline_correct_relevance):.4f}, Incorrect: {np.mean(pipeline_incorrect_relevance):.4f}\")\n",
    "\n",
    "print(f\"\\n{'CONCLUSION'}\")\n",
    "print(\"=\"*40)\n",
    "if joint_class_metrics['accuracy'] > pipeline_class_metrics['accuracy']:\n",
    "    print(\"Joint approach performs better in classification accuracy\")\n",
    "else:\n",
    "    print(\"Pipeline approach performs better in classification accuracy\")\n",
    "\n",
    "if joint_exp_metrics['avg_relevance_to_input'] > pipeline_exp_metrics['avg_relevance_to_input']:\n",
    "    print(\"Joint approach generates more relevant explanations\")\n",
    "else:\n",
    "    print(\"Pipeline approach generates more relevant explanations\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
