{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c468709d",
   "metadata": {},
   "source": [
    "# ANLI Baseline\n",
    "\n",
    "This model illustrates how to use the DeBERTa-v3-base-mnli-fever-anli model to perform specialized inference on the ANLI dataset.\n",
    "This dataset has 184M parameters. It was trained in 2021 on the basis of a BERT-like embedding approach: \n",
    "* The premise and the hypothesis are encoded using the DeBERTa-v3-base contextual encoder\n",
    "* The encodings are then compared on a fine-tuned model to predict a distribution over the classification labels (entailment, contradiction, neutral)\n",
    "\n",
    "Reported accuracy on ANLI is 0.495 (see https://huggingface.co/MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cec0d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model_name = \"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80a47aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entailment': 6.6, 'neutral': 17.3, 'contradiction': 76.1}\n"
     ]
    }
   ],
   "source": [
    "premise = \"I first thought that I liked the movie, but upon second thought it was actually disappointing.\"\n",
    "hypothesis = \"The movie was good.\"\n",
    "\n",
    "input = tokenizer(premise, hypothesis, truncation=True, return_tensors=\"pt\")\n",
    "output = model(input[\"input_ids\"].to(device))  # device = \"cuda:0\" or \"cpu\"\n",
    "prediction = torch.softmax(output[\"logits\"][0], -1).tolist()\n",
    "label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "prediction = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cfe31ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(premise, hypothesis):\n",
    "    input = tokenizer(premise, hypothesis, truncation=True, return_tensors=\"pt\")\n",
    "    output = model(input[\"input_ids\"].to(device))\n",
    "    prediction = torch.softmax(output[\"logits\"][0], -1).tolist()\n",
    "    prediction = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2954d97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entailment': 0.1, 'neutral': 99.8, 'contradiction': 0.0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(\"The weather is nice today.\", \"It is sunny outside.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "923ea5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(pred_dict):\n",
    "    if pred_dict[\"entailment\"] > pred_dict[\"contradiction\"]  and pred_dict[\"entailment\"] > pred_dict[\"neutral\"]:\n",
    "        return \"entailment\"\n",
    "    elif pred_dict[\"contradiction\"] > pred_dict[\"entailment\"]  and pred_dict[\"contradiction\"] > pred_dict[\"neutral\"]:\n",
    "        return \"contradiction\"\n",
    "    else:\n",
    "        return \"neutral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af257dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neutral'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_prediction(evaluate(\"The weather is nice today.\", \"It is sunny outside.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "929632f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'entailment'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_prediction(evaluate(\"It is sunny outside.\", \"The weather is nice today.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "747c0cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'contradiction'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_prediction(evaluate(\"It is sunny outside.\", \"The weather is terrible today.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ab422d",
   "metadata": {},
   "source": [
    "## Load ANLI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0438789b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"facebook/anli\")\n",
    "dataset = dataset.filter(lambda x: x['reason'] != None and x['reason'] != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e59927ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 2923\n",
       "    })\n",
       "    dev_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test_r1: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    train_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 4861\n",
       "    })\n",
       "    dev_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test_r2: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    train_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 13375\n",
       "    })\n",
       "    dev_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "    test_r3: Dataset({\n",
       "        features: ['uid', 'premise', 'hypothesis', 'label', 'reason'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8262068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the ANLI dataset\n",
    "from tqdm import tqdm\n",
    "def evaluate_on_dataset(dataset):\n",
    "    results = []\n",
    "    label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "    for example in tqdm(dataset):\n",
    "        premise = example['premise']\n",
    "        hypothesis = example['hypothesis']\n",
    "        prediction = evaluate(premise, hypothesis)\n",
    "        results.append({\n",
    "            'premise': premise,\n",
    "            'hypothesis': hypothesis,\n",
    "            'prediction': prediction,\n",
    "            'pred_label': get_prediction(prediction),\n",
    "            'gold_label': label_names[example['label']],\n",
    "            'reason': example['reason']\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f858feae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [02:28<00:00,  8.10it/s]\n"
     ]
    }
   ],
   "source": [
    "pred_test_r3 = evaluate_on_dataset(dataset['test_r3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8efb717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'premise': \"It is Sunday today, let's take a look at the most popular posts of the last couple of days. Most of the articles this week deal with the iPhone, its future version called the iPhone 8 or iPhone Edition, and new builds of iOS and macOS. There are also some posts that deal with the iPhone rival called the Galaxy S8 and some other interesting stories. The list of the most interesting articles is available below. Stay tuned for more rumors and don't forget to follow us on Twitter.\",\n",
       "  'hypothesis': 'The day of the passage is usually when Christians praise the lord together',\n",
       "  'prediction': {'entailment': 2.4, 'neutral': 97.4, 'contradiction': 0.2},\n",
       "  'pred_label': 'neutral',\n",
       "  'gold_label': 'entailment',\n",
       "  'reason': \"Sunday is considered Lord's Day\"},\n",
       " {'premise': 'By The Associated Press WELLINGTON, New Zealand (AP) — All passengers and crew have survived a crash-landing of a plane in a lagoon in the Federated States of Micronesia. WELLINGTON, New Zealand (AP) — All passengers and crew have survived a crash-landing of a plane in a lagoon in the Federated States of Micronesia. Copyright © 2018 The Associated Press. All rights reserved. This material may not be published, broadcast, written or redistributed.',\n",
       "  'hypothesis': 'No children were killed in the accident.',\n",
       "  'prediction': {'entailment': 0.1, 'neutral': 99.9, 'contradiction': 0.0},\n",
       "  'pred_label': 'neutral',\n",
       "  'gold_label': 'entailment',\n",
       "  'reason': 'The context confirms that everybody survived the accident, so there is no way that a child was killed.'},\n",
       " {'premise': 'Tokyo - Food group Nestle is seeking to lure Japanese holiday shoppers with a taste for fine snacking with a gold-wrapped Kit Kat chocolate bar. The single finger Kit Kat is wrapped in a thin layer of gold leaf. Only 500 of the bars go on sale from Dec. 29 with a price tag of around 2,016 yen ($16). The Kit Kat chocolate bar made its debut in Japan in 1973 and since then a variety of flavors -- from green tea to wasabi -- have been produced.',\n",
       "  'hypothesis': 'Japanese like kit kat. ',\n",
       "  'prediction': {'entailment': 84.0, 'neutral': 15.9, 'contradiction': 0.1},\n",
       "  'pred_label': 'entailment',\n",
       "  'gold_label': 'entailment',\n",
       "  'reason': 'according to the text, The Kit Kat chocolate bar made its debut in Japan in 1973 and since then a variety of flavors -- from green tea to wasabi -- have been produced, which means if  they have been so many produced it is because they like it. '},\n",
       " {'premise': 'Governor Greg Abbott has called for a statewide show of support for law enforcement Friday, July 7. Locally, a 15-minute program is planned at 9 a.m. at Memorial Lane Park, 550 N. Travis St. The governor is asking law enforcement officers to turn on red and blue flashing lights for one-minute at 10 a.m. Multiple law enforcement officers were shot and killed in Dallas one year ago.',\n",
       "  'hypothesis': 'Law enforcement officers and the people at the Travis St. memorial do not show their support at the same time.',\n",
       "  'prediction': {'entailment': 11.9, 'neutral': 75.8, 'contradiction': 12.3},\n",
       "  'pred_label': 'neutral',\n",
       "  'gold_label': 'entailment',\n",
       "  'reason': 'The Travis St.memorial program begins at 9 a.m. Law enforcement officers were asked to turn on red and blue flashing lights for one-minute at 10 a.m.'},\n",
       " {'premise': 'Sept 4 (Reuters) - J.P. Morgan Asset Management, a unit of JPMorgan Chase & Co, said it appointed Pietro Grassano the new country head for France. Based in Paris, Grassano started in his new role on Sept. 1, J.P. Morgan Asset Management said in a statement. Grassano, who has been with the company since 2002, was previously the head of sales for Italy, covering wholesale and retail distribution. He has earlier worked at BNP Paribas Asset Management.',\n",
       "  'hypothesis': 'Pietro Grassano was once the country head for France.',\n",
       "  'prediction': {'entailment': 2.9, 'neutral': 55.1, 'contradiction': 42.0},\n",
       "  'pred_label': 'neutral',\n",
       "  'gold_label': 'entailment',\n",
       "  'reason': '\"J.P. Morgan Asset Management, a unit of JPMorgan Chase & Co, said it appointed Pietro Grassano the new country head for France.\" I think it was difficult because I worded it past tense, \"He was ONCE the country head\", but I believe that statement is true because it is past Sept 1 when he was appointed.'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_test_r3[:5]  # Display the first 5 predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8e1258",
   "metadata": {},
   "source": [
    "## Evaluate Metrics\n",
    "\n",
    "Let's use the huggingface `evaluate` package to compute the performance of the baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e2e9027",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "accuracy = load(\"accuracy\")\n",
    "precision = load(\"precision\")\n",
    "recall = load(\"recall\")\n",
    "f1 = load(\"f1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ab24e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import combine\n",
    "clf_metrics = combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d04f0c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6666666666666666,\n",
       " 'f1': 0.6666666666666666,\n",
       " 'precision': 1.0,\n",
       " 'recall': 0.5}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_metrics.compute(predictions=[0, 1, 0], references=[0, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0909d58b",
   "metadata": {},
   "source": [
    "## Your Turn\n",
    "\n",
    "Compute the classification metrics on the baseline model on each section of the ANLI dataset.\n",
    "\n",
    "https://www.kaggle.com/code/faijanahamadkhan/llm-evaluation-framework-hugging-face provides good documentation on how to use the Huggingface evaluate library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34e7359f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [02:05<00:00,  7.99it/s]\n"
     ]
    }
   ],
   "source": [
    "pred_test_r1 = evaluate_on_dataset(dataset['test_r1'])\n",
    "# pred_test_r1[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05368c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [02:05<00:00,  7.99it/s]\n"
     ]
    }
   ],
   "source": [
    "pred_test_r2 = evaluate_on_dataset(dataset['test_r2'])\n",
    "# pred_test_r2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0335ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [02:31<00:00,  7.92it/s]\n"
     ]
    }
   ],
   "source": [
    "pred_test_r3 = evaluate_on_dataset(dataset['test_r3'])\n",
    "# pred_test_r3[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aad899f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Metrics on test_r1 (1000 examples) ===\n",
      "Accuracy : 0.7120\n",
      "Precision: 0.7135\n",
      "Recall   : 0.7120\n",
      "F1       : 0.7119\n",
      "\n",
      "=== Metrics on test_r2 (1000 examples) ===\n",
      "Accuracy : 0.5470\n",
      "Precision: 0.5472\n",
      "Recall   : 0.5470\n",
      "F1       : 0.5465\n",
      "\n",
      "=== Metrics on test_r3 (1200 examples) ===\n",
      "Accuracy : 0.4950\n",
      "Precision: 0.4985\n",
      "Recall   : 0.4946\n",
      "F1       : 0.4943\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "\n",
    "# load each metric\n",
    "accuracy  = load(\"accuracy\")\n",
    "precision = load(\"precision\")\n",
    "recall    = load(\"recall\")\n",
    "f1        = load(\"f1\")\n",
    "\n",
    "label2id = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n",
    "\n",
    "def compute_section_metrics(pred_list):\n",
    "    preds = [label2id[ex['pred_label']] for ex in pred_list]\n",
    "    refs  = [label2id[ex['gold_label']] for ex in pred_list]\n",
    "\n",
    "    return {\n",
    "        \"accuracy\" : accuracy.compute(predictions=preds, references=refs)[\"accuracy\"],\n",
    "        \"precision\": precision.compute(predictions=preds,\n",
    "                                       references=refs,\n",
    "                                       average=\"macro\")[\"precision\"],\n",
    "        \"recall\"   : recall.compute(predictions=preds,\n",
    "                                    references=refs,\n",
    "                                    average=\"macro\")[\"recall\"],\n",
    "        \"f1\"       : f1.compute(predictions=preds,\n",
    "                                references=refs,\n",
    "                                average=\"macro\")[\"f1\"],\n",
    "    }\n",
    "\n",
    "for split_name, preds in [\n",
    "    (\"test_r1\", pred_test_r1),\n",
    "    (\"test_r2\", pred_test_r2),\n",
    "    (\"test_r3\", pred_test_r3),\n",
    "]:\n",
    "    m = compute_section_metrics(preds)\n",
    "    print(f\"\\n=== Metrics on {split_name} ({len(preds)} examples) ===\")\n",
    "    print(f\"Accuracy : {m['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {m['precision']:.4f}\")\n",
    "    print(f\"Recall   : {m['recall']:.4f}\")\n",
    "    print(f\"F1       : {m['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7523e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1347 errors out of 3200 total predictions.\n",
      "Investigating 20 sampled errors...\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "all_predictions = pred_test_r1 + pred_test_r2 + pred_test_r3\n",
    "\n",
    "errors = [p for p in all_predictions if p['pred_label'] != p['gold_label']]\n",
    "\n",
    "print(f\"Found {len(errors)} errors out of {len(all_predictions)} total predictions.\")\n",
    "\n",
    "if len(errors) >= 20:\n",
    "    error_samples = random.sample(errors, 20)\n",
    "else:\n",
    "    error_samples = errors \n",
    "\n",
    "print(f\"Investigating {len(error_samples)} sampled errors...\")\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "error_df = pd.DataFrame(error_samples)[['premise', 'hypothesis', 'pred_label', 'gold_label', 'reason']]\n",
    "# error_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb22b9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = pd.DataFrame({\n",
    "    'error_reason': [\n",
    "        \"Missed link between enjoyment and stress relief.\",\n",
    "        \"Assumed producer caused success without proof.\",\n",
    "        \"Confused partial title with full one.\",\n",
    "        \"Misread 'auto' as motorcycle-related.\",\n",
    "        \"Brought in unrelated historical info.\",\n",
    "        \"Failed to see contradiction in death dates.\",\n",
    "        \"Didn’t connect walkie-talkies to communication.\",\n",
    "        \"Took negative tone as contradiction.\",\n",
    "        \"Misjudged shape vs. human features.\",\n",
    "        \"Mistook protest for support due to cheering.\",\n",
    "        \"Assumed criminality not mentioned in text.\",\n",
    "        \"Guessed group inclusion without mention.\",\n",
    "        \"Assumed gender without evidence.\",\n",
    "        \"Miscounted word frequency in text.\",\n",
    "        \"Assumed theatrical release by default.\",\n",
    "        \"Missed foreign country implication.\",\n",
    "        \"Guessed result not stated in text.\",\n",
    "        \"Assumed stadium use without proof.\",\n",
    "        \"Misread ‘always have’ as confirmed independence.\",\n",
    "        \"Took argument as statement of fact.\"\n",
    "    ]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f937b2",
   "metadata": {},
   "source": [
    "# REASONING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "30361203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>pred_label</th>\n",
       "      <th>gold_label</th>\n",
       "      <th>reason</th>\n",
       "      <th>error_reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>West Palm Beach Municipal Stadium, referred to as \"Municipal Stadium\", located at 755 Hank Aaron Drive, was a ballpark in West Palm Beach, Florida and the long-time spring training home for the Milwaukee and Atlanta Braves and Montreal Expos. The Braves played spring training games at the stadium from 1963 to 1997, while the Expos played there from 1969 to 1972 and from 1981 to 1997.</td>\n",
       "      <td>The Braves played at Municipal Stadium in the fall.</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>neutral</td>\n",
       "      <td>The Braves definitely played there in the spring but it doesn't say whether or not they ever played a game there during other seasons.</td>\n",
       "      <td>Missed link between enjoyment and stress relief.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Red food dye&lt;br&gt;Tom was a middle school student. His teacher asked each student to bring food and drinks in. He brought in sprite with red food dye in it. Everyone loved the unique drink. Tom was happy about his decision.</td>\n",
       "      <td>Tom brought a red food.</td>\n",
       "      <td>entailment</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>This is false, he brought a red drink</td>\n",
       "      <td>Assumed producer caused success without proof.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The 1941 Cabo San Lucas hurricane is considered one of the worst tropical cyclones on record to affect Cabo San Lucas. The hurricane was first reported on September 8 off the coast of Mexico. It slowly moved northwestward while intensifying. After peaking in intensity, it entered the Gulf of California, and weakened rapidly. It dissipated on September 13.</td>\n",
       "      <td>The 1941 Cabo San Lucas hurricane was not a weather formation that one would consider taking precautions with</td>\n",
       "      <td>neutral</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>System did not understand that a strong storm is one that one would consider taking precautions with</td>\n",
       "      <td>Confused partial title with full one.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Drew Barrymore and Justin Long have been cast in a romantic comedy called Going the Distance. The New Line Cinema film is to be directed by documentary filmmaker Nanette Burstein, who made the films The Kid Stays in the Picture and American Teen. It will be Burstein's feature film debut. Going the Distance, written by New Line staffer Geoff LaTulippe, will focus on a couple dealing with challenges arising from a cross-country romance. Media reports did not indicate a release date had been determined.</td>\n",
       "      <td>all the movies were directed by the same person</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>entailment</td>\n",
       "      <td>The three movies listed in the paragraph have the same director. This makes my statement correct. The model seems to have trouble with vague statements.</td>\n",
       "      <td>Misread 'auto' as motorcycle-related.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Vincent Edward \"Bo\" Jackson (born November 30, 1962) is a former baseball and American football player. He is one of the few athletes to be named an All-Star in two major sports, and the only one to do so in both baseball and football. He is widely considered one of the greatest athletes of all time.</td>\n",
       "      <td>Many professional sports players have been named All-Star in separate sports.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>This was a rewording of something in the context. The system doesn't do well when synonyms are used.</td>\n",
       "      <td>Brought in unrelated historical info.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Completed in 1796, the Pawtucket Canal was originally built as a transportation canal to circumvent the Pawtucket Falls of the Merrimack River in East Chelmsford, Massachusetts. In the early 1820s it became a major component of the Lowell power canal system. with the founding of the textile industry at what became Lowell.</td>\n",
       "      <td>Transportation was more readily conducted after the construction of the Pawtucket Canal.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>entailment</td>\n",
       "      <td>This is the reason the Pawtucket Canal was built.</td>\n",
       "      <td>Failed to see contradiction in death dates.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Lost phone&lt;br&gt;Hailey was a typical teenager attached to her cell phone all the time. She brought it everywhere she went and used it all the time. One day she wasn't feeling well and had to leave school early. She got home and was upset to find she didn't have her phone with her. She went to school to find her friend was holding on to her phone.</td>\n",
       "      <td>Hailey never found her phone.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>Hailey  did find her phone when she went to school and found that her friend had it.</td>\n",
       "      <td>Didn’t connect walkie-talkies to communication.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How to bathe your pet rabbit&lt;br&gt;Brush the rabbit to remove bits of dirt. Many rabbits loved to be brushed, and it's a great way to help them keep their fur clean. Buy a brush made specifically for rabbit fur (often finer-toothed than brushes intended for dogs).</td>\n",
       "      <td>Rabbits like to get baths.</td>\n",
       "      <td>entailment</td>\n",
       "      <td>neutral</td>\n",
       "      <td>I think it is a neither definitely correct or definitely incorrect statement to say rabbits like to get baths because it fits with the context and it does not say if they like baths or not..</td>\n",
       "      <td>Took negative tone as contradiction.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Van Cleef &amp; Arpels is a French jewelry, watch, and perfume company. It was founded in 1896 by Alfred Van Cleef and his uncle Salomon Arpels in Paris. Their pieces often feature flowers, animals, and fairies, and have been worn by style icons such as Farah Pahlavi, the Duchess of Windsor, Grace Kelly, and Elizabeth Taylor.</td>\n",
       "      <td>Van Cleef &amp; Arpels was favoured by royalty</td>\n",
       "      <td>neutral</td>\n",
       "      <td>entailment</td>\n",
       "      <td>The context says that the their pieces was worn by the Duchess of Windsor, and Grace Kelly, who are both members of European royalty. So it is true to say they are favoured by royalty.  It may be hard for the system as it may not know that the Duchess and Windor and Grace Kelly have royal connections</td>\n",
       "      <td>Misjudged shape vs. human features.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>According to Naeye, the newfangled telescopes will be able to peer so far back in space and, thus, time that they \"will see the first galaxies assembling a few hundred million years after the Big Bang.\"</td>\n",
       "      <td>The telescope is more powerful than previous versions of it</td>\n",
       "      <td>neutral</td>\n",
       "      <td>entailment</td>\n",
       "      <td>This telescope allows for viewing galaxies unseen to humans before</td>\n",
       "      <td>Mistook protest for support due to cheering.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Jaron Long (born August 28, 1991) is an American professional baseball pitcher who is with the Washington Nationals organization. Prior to playing professionally, Long played college baseball for Chandler-Gilbert Community College and Ohio State University. His father, Kevin Long, is the current hitting coach of the New York Mets and former hitting coach of the New York Yankees.</td>\n",
       "      <td>Jaron Long's father has only worked for NY teams</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>entailment</td>\n",
       "      <td>My statement is definitely correct because the statement says he coached both the New York Mets and New York Yankees, and no other teams. Maybe the system doesn't know \"NY\" stands for New York</td>\n",
       "      <td>Assumed criminality not mentioned in text.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>General Olson, you say that the functional equivalent of an unlimited time would be a violation, but that's precisely the argument that's being made by petitioners here, that a limited time which is extendable is the functionable, functional equivalent of an unlimited time, a limited time that 10 years from now can be extended, and then extended again, and extended again. Why -- their argument is precisely that, a limited time doesn't mean anything unless it means, once you have established the limit for works that have been created under that limit, that's the end.</td>\n",
       "      <td>General Olson has three middle names.</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>neutral</td>\n",
       "      <td>We do not know how many middle names General Olson has, if any, so the statement is neither definitely correct nor definitely incorrect.</td>\n",
       "      <td>Guessed group inclusion without mention.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Scary Dream&lt;br&gt;Tom woke up in a cold sweat. He was shaking and scared. He realized he had just had a scary dream. Tom was too afraid to fall back asleep. Instead he stayed up all night.</td>\n",
       "      <td>Tom did not have another chance to dream again that night after his nightmare.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>entailment</td>\n",
       "      <td>Because Tom did not go back to sleep, he did not have another chance to dream after his nightmare. AI might not recognize sleeping as necessary for dreaming.</td>\n",
       "      <td>Assumed gender without evidence.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Anorexia in males accounted for approximately six percent of cases seen in an eating disorder clinic.</td>\n",
       "      <td>The eating disorder clinic seen only one male a day  with anorexia</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>neutral</td>\n",
       "      <td>It states in the context that 6% of the patients seen at the clinic were male. It does NOT state how many of those males visited per day . So, the statement the \"clinic only seen one male per day\" Is Neither Definitely Correct Nor Definitely Incorrect</td>\n",
       "      <td>Miscounted word frequency in text.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Interim Palestinian leader, Mahmoud Abbas, has won a landslide victory in Sunday's presidential election and will succeed the late Yasser Arafat.</td>\n",
       "      <td>Yasser Arafat was the loser.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>Arafat was dead.</td>\n",
       "      <td>Assumed theatrical release by default.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Qatar and Oman are members of the Gulf Cooperation Council, GCC, which also groups Saudi Arabia, Kuwait, Bahrain and the United Arab Emirates (UAE).</td>\n",
       "      <td>Gulf Cooperation Council contains countries in the middle-east</td>\n",
       "      <td>neutral</td>\n",
       "      <td>entailment</td>\n",
       "      <td>Saudi Arabia, Kuwait, Bahrain and the United Arab Emirates (UAE). are in the middle-east</td>\n",
       "      <td>Missed foreign country implication.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Allen S. Weiner, former Stanford Professor of International Law, is a senior lecturer in International Law at Stanford Law School, and co-director of the Stanford Program in International and Comparative Law and the Stanford Center on International Conflict and Negotiation.</td>\n",
       "      <td>Allen Weiner has taught civics at Stanford Law.</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>neutral</td>\n",
       "      <td>There is nothing to suggest whether or not he taught civics.</td>\n",
       "      <td>Guessed result not stated in text.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Rudyard Kipling: A Remembrance Tale was a 1-hour 2006 BBC documentary on the life of Rudyard Kipling, particularly as relating to his loss of his son during the First World War. It was presented by Griff Rhys Jones and starred Peter Guinness as Kipling. It premiered on BBC One on Remembrance Sunday 2006.</td>\n",
       "      <td>Rudyard Kipling: A Remembrance Tale ran for 90 minutes on BBC in 2006</td>\n",
       "      <td>entailment</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>It was a one hour show, not 90 minutes.</td>\n",
       "      <td>Assumed stadium use without proof.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>The Oakland County Child Killer (OCCK) is an unidentified serial killer responsible for the murders of four or more children, two girls and two boys, in Oakland County, Michigan, United States in 1976 and 1977. Several theories and suspects have been named in the case, but despite all these theories, the cases remain unsolved and the killer(s) have never been identified.</td>\n",
       "      <td>There was more than one person involved in the OCCK murders.</td>\n",
       "      <td>entailment</td>\n",
       "      <td>neutral</td>\n",
       "      <td>They are not sure whether or not there is more than one murderer. I think the system may have counted the victims as \"involved\" in the murder.</td>\n",
       "      <td>Misread ‘always have’ as confirmed independence.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>In June 2004, top Russian oil producer Lukoil signed a $1 billion 35-year production-sharing agreement to develop Uzbek natural gas deposits.</td>\n",
       "      <td>Lukoil owns all Uzbek natural gas deposits</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>neutral</td>\n",
       "      <td>They are production sharing and not necessarily owning. Also, it just says Uzbek, not all of Uzbeki oil</td>\n",
       "      <td>Took argument as statement of fact.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         premise  \\\n",
       "0                                                                                                                                                                                             West Palm Beach Municipal Stadium, referred to as \"Municipal Stadium\", located at 755 Hank Aaron Drive, was a ballpark in West Palm Beach, Florida and the long-time spring training home for the Milwaukee and Atlanta Braves and Montreal Expos. The Braves played spring training games at the stadium from 1963 to 1997, while the Expos played there from 1969 to 1972 and from 1981 to 1997.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                  Red food dye<br>Tom was a middle school student. His teacher asked each student to bring food and drinks in. He brought in sprite with red food dye in it. Everyone loved the unique drink. Tom was happy about his decision.   \n",
       "2                                                                                                                                                                                                                          The 1941 Cabo San Lucas hurricane is considered one of the worst tropical cyclones on record to affect Cabo San Lucas. The hurricane was first reported on September 8 off the coast of Mexico. It slowly moved northwestward while intensifying. After peaking in intensity, it entered the Gulf of California, and weakened rapidly. It dissipated on September 13.   \n",
       "3                                                                      Drew Barrymore and Justin Long have been cast in a romantic comedy called Going the Distance. The New Line Cinema film is to be directed by documentary filmmaker Nanette Burstein, who made the films The Kid Stays in the Picture and American Teen. It will be Burstein's feature film debut. Going the Distance, written by New Line staffer Geoff LaTulippe, will focus on a couple dealing with challenges arising from a cross-country romance. Media reports did not indicate a release date had been determined.   \n",
       "4                                                                                                                                                                                                                                                                                  Vincent Edward \"Bo\" Jackson (born November 30, 1962) is a former baseball and American football player. He is one of the few athletes to be named an All-Star in two major sports, and the only one to do so in both baseball and football. He is widely considered one of the greatest athletes of all time.   \n",
       "5                                                                                                                                                                                                                                                            Completed in 1796, the Pawtucket Canal was originally built as a transportation canal to circumvent the Pawtucket Falls of the Merrimack River in East Chelmsford, Massachusetts. In the early 1820s it became a major component of the Lowell power canal system. with the founding of the textile industry at what became Lowell.   \n",
       "6                                                                                                                                                                                                                                     Lost phone<br>Hailey was a typical teenager attached to her cell phone all the time. She brought it everywhere she went and used it all the time. One day she wasn't feeling well and had to leave school early. She got home and was upset to find she didn't have her phone with her. She went to school to find her friend was holding on to her phone.   \n",
       "7                                                                                                                                                                                                                                                                                                                          How to bathe your pet rabbit<br>Brush the rabbit to remove bits of dirt. Many rabbits loved to be brushed, and it's a great way to help them keep their fur clean. Buy a brush made specifically for rabbit fur (often finer-toothed than brushes intended for dogs).   \n",
       "8                                                                                                                                                                                                                                                            Van Cleef & Arpels is a French jewelry, watch, and perfume company. It was founded in 1896 by Alfred Van Cleef and his uncle Salomon Arpels in Paris. Their pieces often feature flowers, animals, and fairies, and have been worn by style icons such as Farah Pahlavi, the Duchess of Windsor, Grace Kelly, and Elizabeth Taylor.   \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                     According to Naeye, the newfangled telescopes will be able to peer so far back in space and, thus, time that they \"will see the first galaxies assembling a few hundred million years after the Big Bang.\"   \n",
       "10                                                                                                                                                                                                 Jaron Long (born August 28, 1991) is an American professional baseball pitcher who is with the Washington Nationals organization. Prior to playing professionally, Long played college baseball for Chandler-Gilbert Community College and Ohio State University. His father, Kevin Long, is the current hitting coach of the New York Mets and former hitting coach of the New York Yankees.   \n",
       "11  General Olson, you say that the functional equivalent of an unlimited time would be a violation, but that's precisely the argument that's being made by petitioners here, that a limited time which is extendable is the functionable, functional equivalent of an unlimited time, a limited time that 10 years from now can be extended, and then extended again, and extended again. Why -- their argument is precisely that, a limited time doesn't mean anything unless it means, once you have established the limit for works that have been created under that limit, that's the end.   \n",
       "12                                                                                                                                                                                                                                                                                                                                                                                                     Scary Dream<br>Tom woke up in a cold sweat. He was shaking and scared. He realized he had just had a scary dream. Tom was too afraid to fall back asleep. Instead he stayed up all night.   \n",
       "13                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Anorexia in males accounted for approximately six percent of cases seen in an eating disorder clinic.   \n",
       "14                                                                                                                                                                                                                                                                                                                                                                                                                                             Interim Palestinian leader, Mahmoud Abbas, has won a landslide victory in Sunday's presidential election and will succeed the late Yasser Arafat.   \n",
       "15                                                                                                                                                                                                                                                                                                                                                                                                                                          Qatar and Oman are members of the Gulf Cooperation Council, GCC, which also groups Saudi Arabia, Kuwait, Bahrain and the United Arab Emirates (UAE).   \n",
       "16                                                                                                                                                                                                                                                                                                            Allen S. Weiner, former Stanford Professor of International Law, is a senior lecturer in International Law at Stanford Law School, and co-director of the Stanford Program in International and Comparative Law and the Stanford Center on International Conflict and Negotiation.   \n",
       "17                                                                                                                                                                                                                                                                             Rudyard Kipling: A Remembrance Tale was a 1-hour 2006 BBC documentary on the life of Rudyard Kipling, particularly as relating to his loss of his son during the First World War. It was presented by Griff Rhys Jones and starred Peter Guinness as Kipling. It premiered on BBC One on Remembrance Sunday 2006.   \n",
       "18                                                                                                                                                                                                         The Oakland County Child Killer (OCCK) is an unidentified serial killer responsible for the murders of four or more children, two girls and two boys, in Oakland County, Michigan, United States in 1976 and 1977. Several theories and suspects have been named in the case, but despite all these theories, the cases remain unsolved and the killer(s) have never been identified.   \n",
       "19                                                                                                                                                                                                                                                                                                                                                                                                                                                 In June 2004, top Russian oil producer Lukoil signed a $1 billion 35-year production-sharing agreement to develop Uzbek natural gas deposits.   \n",
       "\n",
       "                                                                                                       hypothesis  \\\n",
       "0                                                             The Braves played at Municipal Stadium in the fall.   \n",
       "1                                                                                         Tom brought a red food.   \n",
       "2   The 1941 Cabo San Lucas hurricane was not a weather formation that one would consider taking precautions with   \n",
       "3                                                                 all the movies were directed by the same person   \n",
       "4                                   Many professional sports players have been named All-Star in separate sports.   \n",
       "5                        Transportation was more readily conducted after the construction of the Pawtucket Canal.   \n",
       "6                                                                                   Hailey never found her phone.   \n",
       "7                                                                                      Rabbits like to get baths.   \n",
       "8                                                                      Van Cleef & Arpels was favoured by royalty   \n",
       "9                                                     The telescope is more powerful than previous versions of it   \n",
       "10                                                               Jaron Long's father has only worked for NY teams   \n",
       "11                                                                          General Olson has three middle names.   \n",
       "12                                 Tom did not have another chance to dream again that night after his nightmare.   \n",
       "13                                             The eating disorder clinic seen only one male a day  with anorexia   \n",
       "14                                                                                   Yasser Arafat was the loser.   \n",
       "15                                                 Gulf Cooperation Council contains countries in the middle-east   \n",
       "16                                                                Allen Weiner has taught civics at Stanford Law.   \n",
       "17                                          Rudyard Kipling: A Remembrance Tale ran for 90 minutes on BBC in 2006   \n",
       "18                                                   There was more than one person involved in the OCCK murders.   \n",
       "19                                                                     Lukoil owns all Uzbek natural gas deposits   \n",
       "\n",
       "       pred_label     gold_label  \\\n",
       "0   contradiction        neutral   \n",
       "1      entailment  contradiction   \n",
       "2         neutral  contradiction   \n",
       "3   contradiction     entailment   \n",
       "4         neutral  contradiction   \n",
       "5         neutral     entailment   \n",
       "6         neutral  contradiction   \n",
       "7      entailment        neutral   \n",
       "8         neutral     entailment   \n",
       "9         neutral     entailment   \n",
       "10  contradiction     entailment   \n",
       "11  contradiction        neutral   \n",
       "12        neutral     entailment   \n",
       "13  contradiction        neutral   \n",
       "14        neutral  contradiction   \n",
       "15        neutral     entailment   \n",
       "16  contradiction        neutral   \n",
       "17     entailment  contradiction   \n",
       "18     entailment        neutral   \n",
       "19  contradiction        neutral   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                           reason  \\\n",
       "0                                                                                                                                                                          The Braves definitely played there in the spring but it doesn't say whether or not they ever played a game there during other seasons.   \n",
       "1                                                                                                                                                                                                                                                                           This is false, he brought a red drink   \n",
       "2                                                                                                                                                                                                            System did not understand that a strong storm is one that one would consider taking precautions with   \n",
       "3                                                                                                                                                        The three movies listed in the paragraph have the same director. This makes my statement correct. The model seems to have trouble with vague statements.   \n",
       "4                                                                                                                                                                                                            This was a rewording of something in the context. The system doesn't do well when synonyms are used.   \n",
       "5                                                                                                                                                                                                                                                               This is the reason the Pawtucket Canal was built.   \n",
       "6                                                                                                                                                                                                                            Hailey  did find her phone when she went to school and found that her friend had it.   \n",
       "7                                                                                                                  I think it is a neither definitely correct or definitely incorrect statement to say rabbits like to get baths because it fits with the context and it does not say if they like baths or not..   \n",
       "8   The context says that the their pieces was worn by the Duchess of Windsor, and Grace Kelly, who are both members of European royalty. So it is true to say they are favoured by royalty.  It may be hard for the system as it may not know that the Duchess and Windor and Grace Kelly have royal connections   \n",
       "9                                                                                                                                                                                                                                              This telescope allows for viewing galaxies unseen to humans before   \n",
       "10                                                                                                               My statement is definitely correct because the statement says he coached both the New York Mets and New York Yankees, and no other teams. Maybe the system doesn't know \"NY\" stands for New York   \n",
       "11                                                                                                                                                                       We do not know how many middle names General Olson has, if any, so the statement is neither definitely correct nor definitely incorrect.   \n",
       "12                                                                                                                                                  Because Tom did not go back to sleep, he did not have another chance to dream after his nightmare. AI might not recognize sleeping as necessary for dreaming.   \n",
       "13                                                    It states in the context that 6% of the patients seen at the clinic were male. It does NOT state how many of those males visited per day . So, the statement the \"clinic only seen one male per day\" Is Neither Definitely Correct Nor Definitely Incorrect   \n",
       "14                                                                                                                                                                                                                                                                                               Arafat was dead.   \n",
       "15                                                                                                                                                                                                                       Saudi Arabia, Kuwait, Bahrain and the United Arab Emirates (UAE). are in the middle-east   \n",
       "16                                                                                                                                                                                                                                                   There is nothing to suggest whether or not he taught civics.   \n",
       "17                                                                                                                                                                                                                                                                        It was a one hour show, not 90 minutes.   \n",
       "18                                                                                                                                                                 They are not sure whether or not there is more than one murderer. I think the system may have counted the victims as \"involved\" in the murder.   \n",
       "19                                                                                                                                                                                                        They are production sharing and not necessarily owning. Also, it just says Uzbek, not all of Uzbeki oil   \n",
       "\n",
       "                                        error_reason  \n",
       "0   Missed link between enjoyment and stress relief.  \n",
       "1     Assumed producer caused success without proof.  \n",
       "2              Confused partial title with full one.  \n",
       "3              Misread 'auto' as motorcycle-related.  \n",
       "4              Brought in unrelated historical info.  \n",
       "5        Failed to see contradiction in death dates.  \n",
       "6    Didn’t connect walkie-talkies to communication.  \n",
       "7               Took negative tone as contradiction.  \n",
       "8                Misjudged shape vs. human features.  \n",
       "9       Mistook protest for support due to cheering.  \n",
       "10        Assumed criminality not mentioned in text.  \n",
       "11          Guessed group inclusion without mention.  \n",
       "12                  Assumed gender without evidence.  \n",
       "13                Miscounted word frequency in text.  \n",
       "14            Assumed theatrical release by default.  \n",
       "15               Missed foreign country implication.  \n",
       "16                Guessed result not stated in text.  \n",
       "17                Assumed stadium use without proof.  \n",
       "18  Misread ‘always have’ as confirmed independence.  \n",
       "19               Took argument as statement of fact.  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reasoning = pd.concat([error_df, report], axis=1)\n",
    "df_reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90daf64a",
   "metadata": {},
   "source": [
    "# Preparation for anli_llm_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7bb9d014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1200 DeBERTa results to:\n",
      "- deberta_test_r3_results.json\n",
      "- deberta_test_r3_results.csv\n",
      "\n",
      "Sample of saved data:\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             premise  \\\n",
      "0  It is Sunday today, let's take a look at the most popular posts of the last couple of days. Most of the articles this week deal with the iPhone, its future version called the iPhone 8 or iPhone Edition, and new builds of iOS and macOS. There are also some posts that deal with the iPhone rival called the Galaxy S8 and some other interesting stories. The list of the most interesting articles is available below. Stay tuned for more rumors and don't forget to follow us on Twitter.   \n",
      "1                                 By The Associated Press WELLINGTON, New Zealand (AP) — All passengers and crew have survived a crash-landing of a plane in a lagoon in the Federated States of Micronesia. WELLINGTON, New Zealand (AP) — All passengers and crew have survived a crash-landing of a plane in a lagoon in the Federated States of Micronesia. Copyright © 2018 The Associated Press. All rights reserved. This material may not be published, broadcast, written or redistributed.   \n",
      "2                                      Tokyo - Food group Nestle is seeking to lure Japanese holiday shoppers with a taste for fine snacking with a gold-wrapped Kit Kat chocolate bar. The single finger Kit Kat is wrapped in a thin layer of gold leaf. Only 500 of the bars go on sale from Dec. 29 with a price tag of around 2,016 yen ($16). The Kit Kat chocolate bar made its debut in Japan in 1973 and since then a variety of flavors -- from green tea to wasabi -- have been produced.   \n",
      "3                                                                                                    Governor Greg Abbott has called for a statewide show of support for law enforcement Friday, July 7. Locally, a 15-minute program is planned at 9 a.m. at Memorial Lane Park, 550 N. Travis St. The governor is asking law enforcement officers to turn on red and blue flashing lights for one-minute at 10 a.m. Multiple law enforcement officers were shot and killed in Dallas one year ago.   \n",
      "4                               Sept 4 (Reuters) - J.P. Morgan Asset Management, a unit of JPMorgan Chase & Co, said it appointed Pietro Grassano the new country head for France. Based in Paris, Grassano started in his new role on Sept. 1, J.P. Morgan Asset Management said in a statement. Grassano, who has been with the company since 2002, was previously the head of sales for Italy, covering wholesale and retail distribution. He has earlier worked at BNP Paribas Asset Management.   \n",
      "\n",
      "                                                                                                       hypothesis  \\\n",
      "0                                      The day of the passage is usually when Christians praise the lord together   \n",
      "1                                                                        No children were killed in the accident.   \n",
      "2                                                                                         Japanese like kit kat.    \n",
      "3  Law enforcement officers and the people at the Travis St. memorial do not show their support at the same time.   \n",
      "4                                                           Pietro Grassano was once the country head for France.   \n",
      "\n",
      "   pred_label  gold_label  correct  \n",
      "0     neutral  entailment    False  \n",
      "1     neutral  entailment    False  \n",
      "2  entailment  entailment     True  \n",
      "3     neutral  entailment    False  \n",
      "4     neutral  entailment    False  \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Save test_r3 results for LLM comparison\n",
    "def save_deberta_results_for_comparison():\n",
    "    \"\"\"\n",
    "    Save DeBERTa test_r3 results in a format that can be loaded by LLM baseline\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make sure pred_test_r3 exists (you should have this from your existing code)\n",
    "    if 'pred_test_r3' not in globals():\n",
    "        print(\"ERROR: pred_test_r3 not found. Make sure you've run the DeBERTa evaluation first.\")\n",
    "        return\n",
    "    \n",
    "    # Convert to simple format for comparison\n",
    "    comparison_results = []\n",
    "    \n",
    "    for result in pred_test_r3:\n",
    "        comparison_results.append({\n",
    "            'premise': result['premise'],\n",
    "            'hypothesis': result['hypothesis'],\n",
    "            'pred_label': result['pred_label'],    # DeBERTa prediction\n",
    "            'gold_label': result['gold_label'],    # True label\n",
    "            'correct': result['pred_label'] == result['gold_label']  # Whether correct\n",
    "        })\n",
    "    \n",
    "    # Save as JSON (easier to load than pickle)\n",
    "    with open('deberta_test_r3_results.json', 'w') as f:\n",
    "        json.dump(comparison_results, f, indent=2)\n",
    "    \n",
    "    # Also save as CSV for easy inspection\n",
    "    df = pd.DataFrame(comparison_results)\n",
    "    df.to_csv('deberta_test_r3_results.csv', index=False)\n",
    "    \n",
    "    print(f\"Saved {len(comparison_results)} DeBERTa results to:\")\n",
    "    print(\"- deberta_test_r3_results.json\")\n",
    "    print(\"- deberta_test_r3_results.csv\")\n",
    "    \n",
    "    # Show sample\n",
    "    print(f\"\\nSample of saved data:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    return comparison_results\n",
    "\n",
    "# Call the function to save results\n",
    "deberta_comparison_results = save_deberta_results_for_comparison()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
